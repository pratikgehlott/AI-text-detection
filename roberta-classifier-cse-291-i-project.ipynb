{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-29T21:37:17.543404Z","iopub.execute_input":"2023-05-29T21:37:17.544343Z","iopub.status.idle":"2023-05-29T21:37:17.555025Z","shell.execute_reply.started":"2023-05-29T21:37:17.544306Z","shell.execute_reply":"2023-05-29T21:37:17.554086Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/project/wiki_csai.jsonl\n/kaggle/input/project/open_qa.jsonl\n/kaggle/input/project/dataset.csv\n/kaggle/input/stackoverflow/merged_stackoverflow_18_34_Answers.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# IMPORTS\nimport argparse\nimport json\nimport logging\nimport math\nimport torch\nimport os\nimport time\nimport random\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport scipy.stats\nimport pickle\n\n\nimport datasets\nfrom collections import Counter\nfrom datasets import load_dataset, load_metric\nfrom torch.utils.data import DataLoader\nfrom torchmetrics.classification import MulticlassAccuracy, accuracy\nfrom sklearn.metrics import classification_report, f1_score\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import WeightedRandomSampler\nimport numpy as np\nfrom datasets import Dataset, DatasetDict\n\nimport transformers\nfrom accelerate import Accelerator\nfrom huggingface_hub import Repository\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    PretrainedConfig,\n    SchedulerType,\n    default_data_collator,\n    get_scheduler,\n    set_seed,\n)\nfrom collections import Counter\nfrom transformers.utils.versions import require_version","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:37:17.559942Z","iopub.execute_input":"2023-05-29T21:37:17.561804Z","iopub.status.idle":"2023-05-29T21:37:17.574109Z","shell.execute_reply.started":"2023-05-29T21:37:17.561766Z","shell.execute_reply":"2023-05-29T21:37:17.573212Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2023-05-27T22:49:29.222280Z","iopub.execute_input":"2023-05-27T22:49:29.222950Z","iopub.status.idle":"2023-05-27T22:49:30.202743Z","shell.execute_reply.started":"2023-05-27T22:49:29.222913Z","shell.execute_reply":"2023-05-27T22:49:30.201559Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"data_file_path = \"/kaggle/input/project/dataset.csv\"\noutput_dir=\"./\"\nseed=42\n\nlogger = logging.getLogger(__name__)\naccelerator = Accelerator()\n# Make one log on every process with the configuration for debugging.\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    level=logging.INFO,\n)\nlogger.info(accelerator.state)\n\n# Setup logging, we only want one process per machine to log things on the screen.\n# accelerator.is_local_main_process is only True for one process per machine.\nlogger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\nif accelerator.is_local_main_process:\n    datasets.utils.logging.set_verbosity_warning()\n    transformers.utils.logging.set_verbosity_info()\nelse:\n    datasets.utils.logging.set_verbosity_error()\n    transformers.utils.logging.set_verbosity_error()\n\n# If passed along, set the training seed now.\nif seed is not None:\n    set_seed(seed)\n\n# Handle the repository creation\nif accelerator.is_main_process:\n    if output_dir is not None:\n        os.makedirs(output_dir, exist_ok=True)\naccelerator.wait_for_everyone()\n\ndf= pd.read_csv(data_file_path)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:39:26.075654Z","iopub.execute_input":"2023-05-29T19:39:26.076062Z","iopub.status.idle":"2023-05-29T19:39:26.558386Z","shell.execute_reply.started":"2023-05-29T19:39:26.076018Z","shell.execute_reply":"2023-05-29T19:39:26.557385Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.chatgpt_answer[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:43:44.026876Z","iopub.execute_input":"2023-05-29T19:43:44.027359Z","iopub.status.idle":"2023-05-29T19:43:44.034780Z","shell.execute_reply.started":"2023-05-29T19:43:44.027323Z","shell.execute_reply":"2023-05-29T19:43:44.033681Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'To cook clams and mussels for pasta, add the shellfish to the sauce during the last 5-10 minutes of cooking (once the sauce has already been simmering for at least 30 minutes). The clams and mussels will release their Flavor into the sauce as they cook. Be sure to discard any shellfish that do not open after cooking.'"},"metadata":{}}]},{"cell_type":"code","source":"df= df[df[\"origin\"]==\"AskCulinary\"]\ndf.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T00:34:41.953979Z","iopub.execute_input":"2023-05-28T00:34:41.954385Z","iopub.status.idle":"2023-05-28T00:34:41.962476Z","shell.execute_reply.started":"2023-05-28T00:34:41.954352Z","shell.execute_reply":"2023-05-28T00:34:41.961497Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nhuman_answers = df.apply(lambda x: f\"Question: {x['post_question']} , Answer: {x['human_answer']}\" , axis=1)\nchatgpt_answers = df.apply(lambda x: f\"Question: {x['post_question']} , Answer: {x['chatgpt_answer']}\" , axis=1)\n# human_answers= df[\"human_answer\"]\n# chatgpt_answers = df[\"chatgpt_answer\"]\n\nprocessed_df= pd.DataFrame({\"text\": list(human_answers)+ list(chatgpt_answers), \"label\": [0]* len(human_answers) + [1]*len(chatgpt_answers)})\n\ntrain_val_df, test_df= train_test_split(processed_df, test_size = 0.1, stratify=processed_df.label, random_state=0)\ntrain_df, val_df = train_test_split(train_val_df, test_size = 0.1, stratify=train_val_df.label, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T00:34:42.821870Z","iopub.execute_input":"2023-05-28T00:34:42.822524Z","iopub.status.idle":"2023-05-28T00:34:42.989908Z","shell.execute_reply.started":"2023-05-28T00:34:42.822488Z","shell.execute_reply":"2023-05-28T00:34:42.989013Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"sequence_classification_model=\"roberta-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(sequence_classification_model, do_lower_case=True)\nif sequence_classification_model== \"gpt2\":\n    tokenizer.padding_side = \"left\"\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    \ntrain_encodings = tokenizer(train_df.text.tolist(), truncation=True, padding=True, max_length=512)\nval_encodings = tokenizer(val_df.text.tolist(), truncation=True, padding=True, max_length=512)\ntest_encodings = tokenizer(test_df.text.tolist(), truncation=True, padding=True, max_length=512)\n\ntrain_y = list(train_df['label'])\nval_y = list(val_df['label'])\ntest_y = list(test_df['label'])\n\n\n# Create PyTorch datasets\nclass TextClassificationDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, torch.tensor(self.labels[idx])\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = TextClassificationDataset(train_encodings, train_y)\ntest_dataset = TextClassificationDataset(test_encodings, test_y)\nval_dataset = TextClassificationDataset(val_encodings, val_y)\n\n# Create PyTorch data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T00:34:45.945294Z","iopub.execute_input":"2023-05-28T00:34:45.945652Z","iopub.status.idle":"2023-05-28T00:34:50.887219Z","shell.execute_reply.started":"2023-05-28T00:34:45.945622Z","shell.execute_reply":"2023-05-28T00:34:50.886283Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"Could not locate the tokenizer configuration file, will try to use the model config instead.\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\nModel config RobertaConfig {\n  \"_name_or_path\": \"roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.29.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nloading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\nModel config RobertaConfig {\n  \"_name_or_path\": \"roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.29.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.model_selection import train_test_split\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n\n# Load the pre-trained BERT model\nmodel = AutoModelForSequenceClassification.from_pretrained(sequence_classification_model, num_labels=2)\nif sequence_classification_model==\"gpt2\":\n      model.config.pad_token_id = model.config.eos_token_id\n\n# Set the hyperparameters\nlearning_rate = 2e-5\nweight_decay = 0.01\nbatch_size = 32\nnum_epochs = 1\n\n# Create the optimizer and the learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, eps = 1e-7)\ntotal_steps = len(train_loader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=50, num_training_steps=total_steps)\n\n# Train the model\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nprint(f\"The device is: {device}\")\nmodel.to(device)\nfor epoch in range(num_epochs):\n    total_train_loss = 0\n    total_train_accuracy = 0\n    model.train()\n    pred_logits =torch.Tensor()\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        inputs = {key: val.to(device) for key, val in batch[0].items()}\n        labels = batch[1].to(device)\n        outputs = model(**inputs, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        logits = outputs.logits\n        total_train_loss+=loss.item()\n        softmax =torch.softmax(logits, dim=1)\n        pred_logits= torch.cat([pred_logits, softmax.cpu()], dim=0)\n\n        preds = torch.argmax(logits, axis=1)\n        train_accuracy = torch.sum(preds == labels).item() / len(labels)\n        total_train_accuracy += train_accuracy\n    torch.save(pred_logits,f\"train_roberta_{epoch}_ep_pred_tensors.pt\")\n\n    print(f\"epoch: {epoch}, Training Loss: {total_train_loss/len(train_loader)},\\\n     Training accuracy: {total_train_accuracy / len(train_loader)}\")\n    \n    \n    model.eval()\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    pred_class = []\n    for batch in val_loader:\n        with torch.no_grad():\n            inputs = {key: val.to(device) for key, val in batch[0].items()}\n            labels = batch[1].to(device)\n            outputs = model(**inputs, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n\n        total_eval_loss += loss.item()\n        preds = torch.argmax(logits, axis=1)\n        eval_accuracy = torch.sum(preds == labels).item() / len(labels)\n    #     print(eval_accuracy)\n        total_eval_accuracy += eval_accuracy\n\n    avg_eval_accuracy = total_eval_accuracy / len(val_loader)\n    avg_eval_loss = total_eval_loss / len(val_loader)\n\n    print(f\"Epoch {epoch+1}:\")\n    print(f\"  Training loss: {loss.item():.4f}\")\n    print(f\"  Testing loss: {avg_eval_loss:.4f}\")\n    print(f\"  Testing accuracy: {avg_eval_accuracy:.4f}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-28T00:34:50.889447Z","iopub.execute_input":"2023-05-28T00:34:50.889831Z","iopub.status.idle":"2023-05-28T00:46:35.796220Z","shell.execute_reply.started":"2023-05-28T00:34:50.889787Z","shell.execute_reply":"2023-05-28T00:46:35.795071Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\nModel config RobertaConfig {\n  \"_name_or_path\": \"roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.29.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors\nSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"The device is: cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 476/476 [11:17<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 0, Training Loss: 0.16957227854564896,     Training accuracy: 0.9180484693877551\nEpoch 1:\n  Training loss: 0.0962\n  Testing loss: 0.0683\n  Testing accuracy: 0.9775\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\ntotal_eval_accuracy = 0\ntotal_eval_loss = 0\npred_list=[]\nfor batch in test_loader:\n    with torch.no_grad():\n        inputs = {key: val.to(device) for key, val in batch[0].items()}\n        labels = batch[1].to(device)\n        outputs = model(**inputs, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n    total_eval_loss += loss.item()\n    preds = torch.argmax(logits, axis=1)\n    pred_list.extend(list(preds.cpu().numpy()))\n    eval_accuracy = torch.sum(preds == labels).item() / len(labels)\n#     print(eval_accuracy)\n    total_eval_accuracy += eval_accuracy\n\navg_eval_accuracy = total_eval_accuracy / len(test_loader)\navg_eval_loss = total_eval_loss / len(test_loader)\n\nprint(f\"Epoch {epoch+1}:\")\nprint(f\"  Training loss: {loss.item():.4f}\")\nprint(f\"  Testing loss: {avg_eval_loss:.4f}\")\nprint(f\"  Testing accuracy: {avg_eval_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-28T00:46:35.797705Z","iopub.execute_input":"2023-05-28T00:46:35.798277Z","iopub.status.idle":"2023-05-28T00:47:04.199012Z","shell.execute_reply.started":"2023-05-28T00:46:35.798240Z","shell.execute_reply":"2023-05-28T00:47:04.197884Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Epoch 1:\n  Training loss: 0.3812\n  Testing loss: 0.0829\n  Testing accuracy: 0.9701\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_y, pred_list, digits=4))","metadata":{"execution":{"iopub.status.busy":"2023-05-28T00:47:04.201255Z","iopub.execute_input":"2023-05-28T00:47:04.201704Z","iopub.status.idle":"2023-05-28T00:47:04.219511Z","shell.execute_reply.started":"2023-05-28T00:47:04.201658Z","shell.execute_reply":"2023-05-28T00:47:04.218521Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.9826    0.9575    0.9699       471\n           1     0.9585    0.9830    0.9706       470\n\n    accuracy                         0.9702       941\n   macro avg     0.9705    0.9703    0.9702       941\nweighted avg     0.9706    0.9702    0.9702       941\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"./\")\ntokenizer.save_pretrained(\"./\")","metadata":{"execution":{"iopub.status.busy":"2023-05-27T23:20:20.899290Z","iopub.execute_input":"2023-05-27T23:20:20.899690Z","iopub.status.idle":"2023-05-27T23:20:21.806222Z","shell.execute_reply.started":"2023-05-27T23:20:20.899660Z","shell.execute_reply":"2023-05-27T23:20:21.805145Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Configuration saved in ./config.json\nModel weights saved in ./pytorch_model.bin\ntokenizer config file saved in ./tokenizer_config.json\nSpecial tokens file saved in ./special_tokens_map.json\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"('./tokenizer_config.json',\n './special_tokens_map.json',\n './vocab.json',\n './merges.txt',\n './added_tokens.json',\n './tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Test on stackoverflow","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv(data_file_path)\n\ndf= df[df[\"origin\"]==\"StackOverflow\"]\ndf.reset_index(drop=True, inplace=True)\n\nfrom sklearn.model_selection import train_test_split\n# human_answers = df.apply(lambda x: f\"Question: {x['post_question']} , Answer: {x['post_answer']}\" , axis=1)\n# chatgpt_answers = df.apply(lambda x: f\"Question: {x['post_question']} , Answer: {x['chatgpt-3.5-turbo']}\" , axis=1)\n# human_answers= df[\"human_answer\"]\n# chatgpt_answers = df[\"chatgpt_answer\"]\n\nprocessed_df= pd.DataFrame({\"text\": list(human_answers)+ list(chatgpt_answers), \"label\": [0]* len(human_answers) + [1]*len(chatgpt_answers)})\n\ntrain_val_df, test_df= train_test_split(processed_df, test_size = 0.1, stratify=processed_df.label, random_state=0)\ntrain_df, val_df = train_test_split(train_val_df, test_size = 0.1, stratify=train_val_df.label, random_state=0)\n\ntest_encodings = tokenizer(test_df.text.tolist(), truncation=True, padding=True, max_length=512)\ntest_y = list(test_df['label'])\ntest_dataset = TextClassificationDataset(test_encodings, test_y)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n\nmodel.eval()\ntotal_eval_accuracy = 0\ntotal_eval_loss = 0\npred_list=[]\nfor batch in test_loader:\n    with torch.no_grad():\n        inputs = {key: val.to(device) for key, val in batch[0].items()}\n        labels = batch[1].to(device)\n        outputs = model(**inputs, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n    total_eval_loss += loss.item()\n    preds = torch.argmax(logits, axis=1)\n    pred_list.extend(list(preds.cpu().numpy()))\n    eval_accuracy = torch.sum(preds == labels).item() / len(labels)\n#     print(eval_accuracy)\n    total_eval_accuracy += eval_accuracy\n\navg_eval_accuracy = total_eval_accuracy / len(test_loader)\navg_eval_loss = total_eval_loss / len(test_loader)\n\nprint(f\"Epoch {epoch+1}:\")\nprint(f\"  Training loss: {loss.item():.4f}\")\nprint(f\"  Testing loss: {avg_eval_loss:.4f}\")\nprint(f\"  Testing accuracy: {avg_eval_accuracy:.4f}\")\n\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_y, pred_list, digits=4))","metadata":{"execution":{"iopub.status.busy":"2023-05-27T23:39:07.640052Z","iopub.execute_input":"2023-05-27T23:39:07.640550Z","iopub.status.idle":"2023-05-27T23:39:35.843244Z","shell.execute_reply.started":"2023-05-27T23:39:07.640511Z","shell.execute_reply":"2023-05-27T23:39:35.842046Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch 1:\n  Training loss: 0.6224\n  Testing loss: 0.4198\n  Testing accuracy: 0.8484\n              precision    recall  f1-score   support\n\n           0     0.8442    0.8553    0.8497       456\n           1     0.8530    0.8418    0.8473       455\n\n    accuracy                         0.8485       911\n   macro avg     0.8486    0.8485    0.8485       911\nweighted avg     0.8486    0.8485    0.8485       911\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train on open QA test on Wiki","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/project/open_qa.jsonl') as f:\n    lines = f.readlines()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:37:17.578264Z","iopub.execute_input":"2023-05-29T21:37:17.580844Z","iopub.status.idle":"2023-05-29T21:37:17.645712Z","shell.execute_reply.started":"2023-05-29T21:37:17.580813Z","shell.execute_reply":"2023-05-29T21:37:17.644754Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"open_qa_df = pd.read_json('/kaggle/input/project/open_qa.jsonl', lines=True)\nwiki_csai_df = pd.read_json('/kaggle/input/project/wiki_csai.jsonl', lines=True)\n\nfrom sklearn.model_selection import train_test_split\nhuman_answers= open_qa_df[\"human_answers\"].apply(lambda x: x[0])\nchatgpt_answers = open_qa_df[\"chatgpt_answers\"].apply(lambda x: x[0])\n\nprocessed_df= pd.DataFrame({\"text\": list(human_answers)+ list(chatgpt_answers), \"label\": [0]* len(human_answers) + [1]*len(chatgpt_answers)})\ntrain_df= processed_df\n\nhuman_answers= wiki_csai_df[\"human_answers\"].apply(lambda x: x[0])\nchatgpt_answers = wiki_csai_df[\"chatgpt_answers\"].apply(lambda x: x[0])\n\nprocessed_df= pd.DataFrame({\"text\": list(human_answers)+ list(chatgpt_answers), \"label\": [0]* len(human_answers) + [1]*len(chatgpt_answers)})\ntest_df= processed_df\n\nprint(train_df.shape)\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T22:17:51.961116Z","iopub.execute_input":"2023-05-29T22:17:51.961477Z","iopub.status.idle":"2023-05-29T22:17:52.029063Z","shell.execute_reply.started":"2023-05-29T22:17:51.961449Z","shell.execute_reply":"2023-05-29T22:17:52.028118Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"(2374, 2)\n(1684, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T22:17:52.647901Z","iopub.execute_input":"2023-05-29T22:17:52.650389Z","iopub.status.idle":"2023-05-29T22:17:52.659341Z","shell.execute_reply.started":"2023-05-29T22:17:52.650355Z","shell.execute_reply":"2023-05-29T22:17:52.658305Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  Animal cognition encompasses the mental capaci...      0\n1  Human intelligence is the intellectual capabil...      0\n2  The Oxford English Dictionary (OED) is the fir...      0\n3  Oxford University Press (OUP) is the universit...      0\n4  Artificial intelligence (AI) has been used in ...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Animal cognition encompasses the mental capaci...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Human intelligence is the intellectual capabil...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Oxford English Dictionary (OED) is the fir...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Oxford University Press (OUP) is the universit...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Artificial intelligence (AI) has been used in ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, test_df= train_test_split(train_df, test_size=0.2, stratify=train_df.label, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T22:17:58.618906Z","iopub.execute_input":"2023-05-29T22:17:58.619275Z","iopub.status.idle":"2023-05-29T22:17:58.631813Z","shell.execute_reply.started":"2023-05-29T22:17:58.619246Z","shell.execute_reply":"2023-05-29T22:17:58.630643Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"sequence_classification_model=\"roberta-base\"\n\nimport torch\nfrom tqdm import tqdm\nimport pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.model_selection import train_test_split\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(sequence_classification_model, do_lower_case=True)\nif sequence_classification_model== \"gpt2\":\n    tokenizer.padding_side = \"left\"\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    \ntrain_encodings = tokenizer(train_df.text.tolist(), truncation=True, padding=True, max_length=512)\ntest_encodings = tokenizer(test_df.text.tolist(), truncation=True, padding=True, max_length=512)\n\ntrain_y = list(train_df['label'])\ntest_y = list(test_df['label'])\n\n\n# Create PyTorch datasets\nclass TextClassificationDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, torch.tensor(self.labels[idx])\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = TextClassificationDataset(train_encodings, train_y)\ntest_dataset = TextClassificationDataset(test_encodings, test_y)\n\n# Create PyTorch data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T22:17:58.994534Z","iopub.execute_input":"2023-05-29T22:17:58.995471Z","iopub.status.idle":"2023-05-29T22:18:00.345532Z","shell.execute_reply.started":"2023-05-29T22:17:58.995428Z","shell.execute_reply":"2023-05-29T22:18:00.344387Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.model_selection import train_test_split\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n\n# Load the pre-trained BERT model\nmodel = AutoModelForSequenceClassification.from_pretrained(sequence_classification_model, num_labels=2)\nif sequence_classification_model==\"gpt2\":\n      model.config.pad_token_id = model.config.eos_token_id\n\n# Set the hyperparameters\nlearning_rate = 2e-5\nweight_decay = 0.01\nbatch_size = 32\nnum_epochs = 2\n\n# Create the optimizer and the learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, eps = 1e-7)\ntotal_steps = len(train_loader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=50, num_training_steps=total_steps)\n\n# Train the model\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nprint(f\"The device is: {device}\")\nmodel.to(device)\nfor epoch in range(num_epochs):\n    total_train_loss = 0\n    total_train_accuracy = 0\n    model.train()\n    pred_logits =torch.Tensor()\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        inputs = {key: val.to(device) for key, val in batch[0].items()}\n        labels = batch[1].to(device)\n        outputs = model(**inputs, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        logits = outputs.logits\n        total_train_loss+=loss.item()\n        softmax =torch.softmax(logits, dim=1)\n        pred_logits= torch.cat([pred_logits, softmax.cpu()], dim=0)\n\n        preds = torch.argmax(logits, axis=1)\n        train_accuracy = torch.sum(preds == labels).item() / len(labels)\n        total_train_accuracy += train_accuracy\n    torch.save(pred_logits,f\"train_roberta_{epoch}_ep_pred_tensors.pt\")\n\n    print(f\"epoch: {epoch}, Training Loss: {total_train_loss/len(train_loader)},\\\n     Training accuracy: {total_train_accuracy / len(train_loader)}\")\n    \n    \n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-29T22:18:01.416224Z","iopub.execute_input":"2023-05-29T22:18:01.416584Z","iopub.status.idle":"2023-05-29T22:23:51.020714Z","shell.execute_reply.started":"2023-05-29T22:18:01.416553Z","shell.execute_reply":"2023-05-29T22:23:51.019765Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"The device is: cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [02:54<00:00,  1.47s/it]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 0, Training Loss: 0.3186355187017627,     Training accuracy: 0.8629201680672269\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [02:53<00:00,  1.46s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 1, Training Loss: 0.02582744897945839,     Training accuracy: 0.9942226890756303\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-28T20:24:38.845422Z","iopub.execute_input":"2023-05-28T20:24:38.846018Z","iopub.status.idle":"2023-05-28T20:24:38.871707Z","shell.execute_reply.started":"2023-05-28T20:24:38.845976Z","shell.execute_reply":"2023-05-28T20:24:38.870663Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"0    842\n1    842\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"model.eval()\ntotal_eval_accuracy = 0\ntotal_eval_loss = 0\npred_list=[]\nfor batch in test_loader:\n    with torch.no_grad():\n        inputs = {key: val.to(device) for key, val in batch[0].items()}\n        labels = batch[1].to(device)\n        outputs = model(**inputs, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n    total_eval_loss += loss.item()\n    preds = torch.argmax(logits, axis=1)\n    pred_list.extend(list(preds.cpu().numpy()))\n    eval_accuracy = torch.sum(preds == labels).item() / len(labels)\n#     print(eval_accuracy)\n    total_eval_accuracy += eval_accuracy\n\navg_eval_accuracy = total_eval_accuracy / len(test_loader)\navg_eval_loss = total_eval_loss / len(test_loader)\n\nprint(f\"Epoch {epoch+1}:\")\nprint(f\"  Training loss: {loss.item():.4f}\")\nprint(f\"  Testing loss: {avg_eval_loss:.4f}\")\nprint(f\"  Testing accuracy: {avg_eval_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-29T22:23:51.022532Z","iopub.execute_input":"2023-05-29T22:23:51.023476Z","iopub.status.idle":"2023-05-29T22:24:00.789340Z","shell.execute_reply.started":"2023-05-29T22:23:51.023439Z","shell.execute_reply":"2023-05-29T22:24:00.788358Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch 2:\n  Training loss: 0.0014\n  Testing loss: 0.0064\n  Testing accuracy: 0.9979\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_y, pred_list, digits=4))","metadata":{"execution":{"iopub.status.busy":"2023-05-29T22:24:00.790912Z","iopub.execute_input":"2023-05-29T22:24:00.791273Z","iopub.status.idle":"2023-05-29T22:24:00.808762Z","shell.execute_reply.started":"2023-05-29T22:24:00.791239Z","shell.execute_reply":"2023-05-29T22:24:00.807903Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.9958    1.0000    0.9979       238\n           1     1.0000    0.9958    0.9979       237\n\n    accuracy                         0.9979       475\n   macro avg     0.9979    0.9979    0.9979       475\nweighted avg     0.9979    0.9979    0.9979       475\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Token perplexity","metadata":{}},{"cell_type":"code","source":"data_file_path = \"/kaggle/input/project/dataset.csv\"\ndf= pd.read_csv(data_file_path)\ndf= df[df[\"origin\"]==\"AskCulinary\"]\ndf.reset_index(drop=True, inplace=True)\n\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nimport torch\nfrom tqdm import tqdm\n\ndevice = \"cuda\"\nmodel_id = \"gpt2\"\nmodel = GPT2LMHeadModel.from_pretrained(model_id).to(device)\ntokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n\ndef calculate_perplexity(text):\n    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(input_ids)\n        log_probabilities = outputs.logits\n        n_tokens = log_probabilities.shape[1]\n        print(log_probabilities.shape)\n        avg_log_probabilities = log_probabilities.sum(dim=-1) / n_tokens\n        print(avg_log_probabilities.shape)\n        perplexity = torch.exp(avg_log_probabilities)\n     \n    return perplexity.item()\n\ntexts = list(df.human_answer)\nperplexities = [calculate_perplexity(text) for text in tqdm(texts)]\nprint(perplexities)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T22:11:20.863335Z","iopub.execute_input":"2023-05-28T22:11:20.864253Z","iopub.status.idle":"2023-05-28T22:11:23.644556Z","shell.execute_reply.started":"2023-05-28T22:11:20.864218Z","shell.execute_reply":"2023-05-28T22:11:23.643276Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"\n\n\n  0%|          | 0/4701 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 104, 50257])\ntorch.Size([1, 104])\ntensor(inf, device='cuda:0')\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[59], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m perplexity\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mhuman_answer)\n\u001b[0;32m---> 29\u001b[0m perplexities \u001b[38;5;241m=\u001b[39m [calculate_perplexity(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(texts)]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(perplexities)\n","Cell \u001b[0;32mIn[59], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m perplexity\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mhuman_answer)\n\u001b[0;32m---> 29\u001b[0m perplexities \u001b[38;5;241m=\u001b[39m [\u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(texts)]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(perplexities)\n","Cell \u001b[0;32mIn[59], line 26\u001b[0m, in \u001b[0;36mcalculate_perplexity\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     24\u001b[0m     perplexity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(avg_log_probabilities)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(perplexity\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mperplexity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 104 elements cannot be converted to Scalar"],"ename":"RuntimeError","evalue":"a Tensor with 104 elements cannot be converted to Scalar","output_type":"error"}]},{"cell_type":"markdown","source":"# Paraphrasing\n","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git","metadata":{"execution":{"iopub.status.busy":"2023-05-28T01:15:30.382000Z","iopub.execute_input":"2023-05-28T01:15:30.382374Z","iopub.status.idle":"2023-05-28T01:15:48.580956Z","shell.execute_reply.started":"2023-05-28T01:15:30.382331Z","shell.execute_reply":"2023-05-28T01:15:48.579850Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git\n  Cloning https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git to /tmp/pip-req-build-_fcub085\n  Running command git clone --filter=blob:none --quiet https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git /tmp/pip-req-build-_fcub085\n  Resolved https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git to commit 720a87a1ee557d8ed8d9a021adbdd1dd5616c5f9\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from parrot==1.0) (4.29.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from parrot==1.0) (0.1.99)\nRequirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.10/site-packages (from parrot==1.0) (0.21.0)\nCollecting sentence-transformers (from parrot==1.0)\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.10/site-packages (from parrot==1.0) (0.18.0)\nRequirement already satisfied: Levenshtein==0.21.0 in /opt/conda/lib/python3.10/site-packages (from python-Levenshtein->parrot==1.0) (0.21.0)\nRequirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from Levenshtein==0.21.0->python-Levenshtein->parrot==1.0) (3.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->parrot==1.0) (4.64.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->parrot==1.0) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->parrot==1.0) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->parrot==1.0) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->parrot==1.0) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->parrot==1.0) (1.10.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->parrot==1.0) (3.2.4)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->parrot==1.0) (0.14.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->parrot==1.0) (3.12.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->parrot==1.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->parrot==1.0) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->parrot==1.0) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->parrot==1.0) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->parrot==1.0) (0.13.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->parrot==1.0) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->parrot==1.0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers->parrot==1.0) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers->parrot==1.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers->parrot==1.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers->parrot==1.0) (3.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers->parrot==1.0) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->parrot==1.0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->parrot==1.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->parrot==1.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->parrot==1.0) (2023.5.7)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->parrot==1.0) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->parrot==1.0) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers->parrot==1.0) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers->parrot==1.0) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers->parrot==1.0) (1.3.0)\nBuilding wheels for collected packages: parrot, sentence-transformers\n  Building wheel for parrot (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for parrot: filename=parrot-1.0-py3-none-any.whl size=8610 sha256=3594b498eb3d00e8554e18d17c8ba076400fde6e4e7b78543d1f73a72b5c6cf7\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ixeytjh_/wheels/e8/ee/2a/4d6a4b2a5c37f5f750e90fa79d2ad84f444fba9b050ecbbe6d\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=7f511134a5bd2c6f2b4166a605395109d7f60183eef98882094ae69dbc9eed2a\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built parrot sentence-transformers\nInstalling collected packages: sentence-transformers, parrot\nSuccessfully installed parrot-1.0 sentence-transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from parrot import Parrot\nimport torch\nimport warnings\nfrom tqdm import tqdm\nwarnings.filterwarnings(\"ignore\")\n\nparrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=True)\n\n\nparaphrased_list=[]\nfor phrase in tqdm(list(df[\"chatgpt_answer\"])):\n    para_phrases = parrot.augment(input_phrase=phrase,max_return_phrases = 1, use_gpu=True)\n    if para_phrases is not None:\n        paraphrased_list.append(para_phrases[0][0])\n    else:\n        paraphrased_list.append(None)\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"para_phrases = parrot.augment(input_phrase=list(df[\"chatgpt_answer\"])[0],max_return_phrases = 2)\nprint(para_phrases)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T01:26:56.494872Z","iopub.execute_input":"2023-05-28T01:26:56.495289Z","iopub.status.idle":"2023-05-28T01:27:03.820657Z","shell.execute_reply.started":"2023-05-28T01:26:56.495256Z","shell.execute_reply":"2023-05-28T01:27:03.819519Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Generate config GenerationConfig {\n  \"_from_model_config\": true,\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0,\n  \"transformers_version\": \"4.29.2\"\n}\n\n","output_type":"stream"},{"name":"stdout","text":"[('To cook clams and mussels for pasta, add the shellfish to the sauce during the last 5-10 minutes of cooking (once the sauce has already been simmering for at least 30 minutes). The clams and mussels will release their Flavor into the sauce as they cook. Be sure to discard any shellfish that do not open after cooking.', 0)]\n","output_type":"stream"}]},{"cell_type":"code","source":"list(df[\"chatgpt_answer\"])[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-28T01:27:15.688369Z","iopub.execute_input":"2023-05-28T01:27:15.688799Z","iopub.status.idle":"2023-05-28T01:27:15.702353Z","shell.execute_reply.started":"2023-05-28T01:27:15.688762Z","shell.execute_reply":"2023-05-28T01:27:15.701379Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'To cook clams and mussels for pasta, add the shellfish to the sauce during the last 5-10 minutes of cooking (once the sauce has already been simmering for at least 30 minutes). The clams and mussels will release their Flavor into the sauce as they cook. Be sure to discard any shellfish that do not open after cooking.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}